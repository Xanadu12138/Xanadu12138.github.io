<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Xanadu</title>
  
  
  <link href="https://xanadu12138.github.io/atom.xml" rel="self"/>
  
  <link href="https://xanadu12138.github.io/"/>
  <updated>2021-08-13T07:19:32.034Z</updated>
  <id>https://xanadu12138.github.io/</id>
  
  <author>
    <name>Yucong Dai</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Thoughts about deeplearning</title>
    <link href="https://xanadu12138.github.io/2021/08/13/thoughts-about-deeplearning/"/>
    <id>https://xanadu12138.github.io/2021/08/13/thoughts-about-deeplearning/</id>
    <published>2021-08-13T03:03:32.000Z</published>
    <updated>2021-08-13T07:19:32.034Z</updated>
    
    <content type="html"><![CDATA[<p>Deep learning is quite popular in computer science. Some think deep learning model is a black box. However, I think some part of deel learning model can be explicitly explained.</p><h1>What is deep learning?</h1><p><img src="./MLvsDL.png" alt="MLvsDL"><br>Compared with conventional machine learning, deep learning models have much more complex network architecture and act more like human. Why am I say that the deep learning models are anthropoid? Deep learning models are unlike conventional machine learning, which is fed by handcraft features and apply mathematical algorithms to cluster or classfy the features. Deep learning models are able to learn what feature is vital for the specific task and some method such as pretraining, fine-tune strategy, mutiple loss fuction etc. to improve the models’ capabilities of learning and take advanteges of the features.</p><h2 id="Process-of-deep-learning">Process of deep learning</h2><p>In my opinion, deep learning models act like <strong>human’s neural network</strong>. It is why the deep learning is called neural network as well. In one sentance, deep learning model takes input and project the input to a high dimensional feature in a learnt way and loss function tell models how to learn project the input. It is analogous to a children learn how to recognize the world. Taking a simple problem as a example, classfy the numbers, the nerual network takes number’s pictures and output what it is. In the training period, the softmax loss function tell the model how to adjust the weight based on the output that model gives. The parents are like loss function, telling children whether they are right or wrong, and help them correct the answer.</p><h2 id="Thoughts-about-convolution-loss-and-architectures">Thoughts about convolution, loss and architectures.</h2><h3 id="Convolution">Convolution</h3><p>In deep learning, a convolutional neural network is a class of artificaial neural network, most commonly applierd to analyze visual imagery. The goal of convolutional neural network is to find the optimized weight of a group of filters in order to render the best features.<br><img src="./convolution.gif" alt="convolution"><br>This image shows how to compute convolution. In traditonal machine learning, the filters are finely designed by human to extract features that are best suited for the task. On the contary, deep learning learn the weights of filters.</p><h3 id="Loss-function">Loss function</h3><p>Loss function tell the neural network the quality of the output it gives. As a good teacher for children, a well designed loss function or mutiple loss functions allow the network to learn better feature representation.</p><p>The mostly used loss functions:</p><h4 id="Softmax-loss-function">Softmax loss function</h4><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mi>t</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>s</mi><mi>w</mi><mo stretchy="false">)</mo></mrow><mrow><munder><mo>∑</mo><mrow><mi>w</mi><mtext>′</mtext><mo>∈</mo><mi>V</mi></mrow></munder><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>s</mi><mi>w</mi><mtext>′</mtext><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">Pt(w) = \cfrac{\exp(sw)}{\sum_{w′∈V}\exp(sw′)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">Pt</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.60308em;vertical-align:-1.01308em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5899999999999999em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.17862099999999992em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mtight">′</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.32708000000000004em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord">′</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop">exp</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.01308em;"><span></span></span></span></span></span><span></span></span></span></span></span></span></p><p>Softmax loss function is commonly used in classf problem. It is simple and elegant. It is good for optimize the distence between inter-class, but it is weak to optimize the distence of intra-class. As a result, there are a lot of variants, such as weighted softmax, Large-margin Softmax and so on. Additionally, softmax loss can be added a temperature hyper-parameter where it can alter the contribution of the output. The bigger temperature make the contribution smoother, the smaller temperature make the contribution sharper.</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mi>t</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>s</mi><mi>w</mi><mi mathvariant="normal">/</mi><mi>t</mi><mo stretchy="false">)</mo></mrow><mrow><munder><mo>∑</mo><mrow><mi>w</mi><mtext>′</mtext><mo>∈</mo><mi>V</mi></mrow></munder><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>s</mi><mi>w</mi><mtext>′</mtext><mi mathvariant="normal">/</mi><mi>t</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">Pt(w) = \cfrac{\exp(sw/t)}{\sum_{w′∈V}\exp(sw′/t)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">Pt</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.60308em;vertical-align:-1.01308em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5899999999999999em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.17862099999999992em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mtight">′</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.32708000000000004em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord">′/</span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop">exp</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord">/</span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.01308em;"><span></span></span></span></span></span><span></span></span></span></span></span></span></p><h4 id="Metric-learning">Metric learning</h4><p>Pairwise Loss and Triplet Loss are commonly used in metric learning. Those loss functions usually pull the postive samples together and push away postive and negative samples.</p><h3 id="Architectures">Architectures</h3><h4 id="Simple">Simple</h4><ol><li>LeNet<br>This network only contains two convolutional layers. It is limited by the ability of computation, it are not able to handle high-resolution images.</li></ol><h4 id="Complex">Complex</h4><p>With the development of computation, it allows to train more complex network. The network are more and more complex and deeper. However, it brings many problem, such as gradience explode or dissaper. ResNet comes to picture to solve this problem. Recently, vision transformer is getting more attention, and attention mechanism get into computer vision area.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Deep learning is quite popular in computer science. Some think deep learning model is a black box. However, I think some part of deel lea</summary>
      
    
    
    
    <category term="Note" scheme="https://xanadu12138.github.io/categories/Note/"/>
    
    
    <category term="Deep learning" scheme="https://xanadu12138.github.io/tags/Deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>Locally Aware Transformer</title>
    <link href="https://xanadu12138.github.io/2021/07/27/la-transformer/"/>
    <id>https://xanadu12138.github.io/2021/07/27/la-transformer/</id>
    <published>2021-07-27T08:21:40.000Z</published>
    <updated>2021-07-27T09:58:21.314Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Review-of-Person-Re-Identification-with-a-Locally-Aware-Transformer"><a href="#Review-of-Person-Re-Identification-with-a-Locally-Aware-Transformer" class="headerlink" title="Review of Person Re-Identification with a Locally Aware Transformer"></a>Review of Person Re-Identification with a Locally Aware Transformer</h2><h3 id="Summary-in-one-sentence"><a href="#Summary-in-one-sentence" class="headerlink" title="Summary in one sentence."></a>Summary in one sentence.</h3><p>Sharma et al. propused a locally aware transormer inspired by Parts-based Convolution Baseline(PCB) and ViT(Vision Transformer) as well as a fine-tunning strategy which further improve Re-ID accuracy</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>Person Re-Identification task:</p><ul><li>Person Re-ID task includes object dectation and image retrival tasks in earily researches. It separates the tasks and focuses on finding the most similar images in gallery according to the given querry image in recent researches.</li><li>I sammarized some common approaches in my last <a href="https://xanadu12138.github.io/2021/07/21/Re-Id/">post</a>, you can check it before going on.</li><li>Extant problem: Exiting approaches only focus on the classification token, and local tokens which are also outputs of the transformer encoder are able to improve the performance of many computer vision tasks.</li></ul><p>The LA-transformer utilize local tokens as an enhanced feature representation of the original image. Inspired by the PCB which partitions the feature vector into six vertical regions and constructs an ensemble of regional classifiers with a voting strategy to determine the predicted class label, LA-transformer adopt a PCB-like strategy to fine-tunning on person Re-Id. A limitation of PCB is that each regional classifier ignores the global information which is also very important for recognition and identification. Thus, LA-transformer combines the CLS token and local token to improve performance.</p><h2 id="Architecture-of-LA-transformer"><a href="#Architecture-of-LA-transformer" class="headerlink" title="Architecture of LA-transformer"></a>Architecture of LA-transformer</h2><p><img src="./top.png" alt="img"><br>The backbone ViT excludes MLP Head and classifier are same as the original ViT. The novel Locally Aware Network is attached on the backbone network. As the image illustrates that Local feature plus global CLS token to form Globally Enhanced Local Tokens and seprates them into 14 parts. After average pooling, each part of GELT are feed to an independed classifier as PCB’s voting strategy. Eventually, using softmax loss to find the optimal feature representation of images.</p><h2 id="Fine-tunning-strategy"><a href="#Fine-tunning-strategy" class="headerlink" title="Fine-tunning strategy."></a>Fine-tunning strategy.</h2><p>Person re-ID datasets are known for their small size and training a transformer on these datasets can quickly lead to overfitting. ViT was pre-trained on ImageNet, and then fine-tuned on person re-ID datasets.</p><p>In blockwise fine-tuning, all transformer blocks are frozen in the start<br>except for the bottleneck model. After every t epochs (where t is a hyper-parameter), one additional transformer encoder block is unfrozen and the learning rate is reduced as described by algorithm1. The learning rate decay helps in reducing the gradient flow in the subsequent layers hence prevent abrupt weight updates.<br><img src="./alg1.png" alt="img"></p><h2 id="Test-architecture"><a href="#Test-architecture" class="headerlink" title="Test architecture"></a>Test architecture</h2><p>In the test period, remove the classifiers and use the GELT as a image’s feature representation. Given a querry image, search the most similar images in gallery by the features learned by models.</p>]]></content>
    
    
    <summary type="html">Reproduction of Person Re-Identification with a Locally Aware Transformer.</summary>
    
    
    
    <category term="Note" scheme="https://xanadu12138.github.io/categories/Note/"/>
    
    
    <category term="Re-Id" scheme="https://xanadu12138.github.io/tags/Re-Id/"/>
    
    <category term="Deep learning" scheme="https://xanadu12138.github.io/tags/Deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>Summary of approaches used in person Re-Id task.(part 1)</title>
    <link href="https://xanadu12138.github.io/2021/07/21/Re-Id/"/>
    <id>https://xanadu12138.github.io/2021/07/21/Re-Id/</id>
    <published>2021-07-21T12:57:10.000Z</published>
    <updated>2021-07-25T02:49:22.387Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Deep-ReID-Architecture-Types"><a href="#Deep-ReID-Architecture-Types" class="headerlink" title="Deep ReID Architecture Types"></a>Deep ReID Architecture Types</h2><h3 id="Classification-Models"><a href="#Classification-Models" class="headerlink" title="Classification Models"></a>Classification Models</h3><p>Classification models consider ReID as a multi-class classification problem. Those models use the softmax loss to predict the class of an input query. Softmax loss encourages the separation of different classes but struggles with large intra-class variations.</p><p>Several methods overcome the inability of softmax loss to handle intra-class variations by introducing new loss.</p><ul><li><p>In “A loss combination based deep model for person re-identification”, Zhu et al. conjunct with center loss which was originally used for facial recognition. AUthors train a CNN with the proposed combination of softmax and center loss to extract discriminative features.<br><img src="fig1.png" alt="Fig1" title="CNN training based on combination of Softmax and centet loss."></p></li><li><p>In “SphereRe-Id: Deep hypersphere manifold embedding for person re-identification”, Fan et al. use a multi-loss training setup having a combination of softmax loss, center loss and “inter-center loss”. While the softmax loss differentiates between different identity samples, the center loss pull the same class identities closer to their center and the inter-center loss push the center of different identity away from another.<br><img src="fig2.png" alt="Fig2"></p></li></ul><h3 id="Verification-Models"><a href="#Verification-Models" class="headerlink" title="Verification Models"></a>Verification Models</h3><p>Verification Models consider ReID to be a binary-classification problem. However, those models suffer from the class imbalance problem.</p><h3 id="Triplet-Based-Re-Id-Models"><a href="#Triplet-Based-Re-Id-Models" class="headerlink" title="Triplet Based Re-Id Models"></a>Triplet Based Re-Id Models</h3><p>Triplet models for Re-Id take triplet input units. Each triplet unit contains three image samples: the anchor(have same identity as the anchor), a positive sample and a negative sample(different identity from the anchor). The triplet loss is trained to keep the Euclidean distance between anchor and positive sample less than anchor and negative sample.<br><img src="fig3.png" alt="fig3"></p><p>Traditional triplet loss has convergence issus.</p><h3 id="part-based-Re-Id-Models"><a href="#part-based-Re-Id-Models" class="headerlink" title="part-based Re-Id Models"></a>part-based Re-Id Models</h3><p>Part-based Re-Id methods extract different image regions to find discriminative part-level features. Thoese models have better performence in handling small inter-class variations such as identifying different people wearing same color clothes due to their superior discrimination capability based on finer part-level cues which are usually suppressed while extracting global features.</p><p>In “Multi-level attention model for person re-identification”, Yan et al. propose a feature attention block for part-based Re-Id. The authors slice features maps into spatial features and assign them weight to highlight the important part regions.<br><img src="fig4.png" alt="fig4"></p><h3 id="Attention-Based-Re-Id-Models"><a href="#Attention-Based-Re-Id-Models" class="headerlink" title="Attention-Based Re-Id Models"></a>Attention-Based Re-Id Models</h3><p>Attention Modules focus on extracting regions containing highly discriminative features while ignoring other regions having little or no discriminative capability.<br><img src="fig5.png" alt="fig5"></p><h2 id="Re-Id-challenges-and-solutions"><a href="#Re-Id-challenges-and-solutions" class="headerlink" title="Re-Id challenges and solutions"></a>Re-Id challenges and solutions</h2><p>The task of person re-identification has faced several challenges like sample variations in view, pose, lightning and scale, partial or complete occlusion, background clutter etc.<br><img src="fig6.png" alt="fig6"></p><ul><li><p>Several deep Re-Id contributions have aimed to develop robust methodoligies against these Re-Id challenges.</p></li><li><p>Skeleton joints data and clothe colors produce pose and lightning invariance, learning view-specific representations for view invariance, utilizing foreground attentive network to suppress noisy background, convolving with multi-scale input to obtain scale invariant features and using pose estimation to achieve pose invariance are some of efforts to overcome these challenges.</p></li></ul>]]></content>
    
    
    <summary type="html">Introduction to common solutions of person Re-Id task.</summary>
    
    
    
    <category term="Note" scheme="https://xanadu12138.github.io/categories/Note/"/>
    
    
    <category term="Re-Id" scheme="https://xanadu12138.github.io/tags/Re-Id/"/>
    
    <category term="Deep learning" scheme="https://xanadu12138.github.io/tags/Deep-learning/"/>
    
  </entry>
  
</feed>
